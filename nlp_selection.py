# -*- coding: utf-8 -*-
"""Untitled26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IATJQLv3rtNifjKb5MlViTU-rlfmsVXA
"""

!pip install datasets transformers nltk

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import torch.nn.functional as F

import datasets
from transformers import BertTokenizer
import nltk

import math
import random
import numpy as np
from tqdm import tqdm

nltk.download('punkt')
nltk.download('punkt_tab')

print("All libraries installed and imported successfully.")


CONFIG = {
    "max_seq_len": 128,
    "batch_size": 16,
    "learning_rate": 5e-5,
    "num_train_steps": 200,
    "num_eval_steps": 50,

    "hidden_dim": 256,
    "num_layers": 4,
    "num_heads": 8,
    "ff_dim": 1024,
    "dropout_prob": 0.1,

    "mask_prob": 0.15,
    "limit_dataset_size": 20000
}

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
CONFIG['vocab_size'] = tokenizer.vocab_size


class BertEmbeddings(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.token_embed = nn.Embedding(config['vocab_size'], config['hidden_dim'], padding_idx=tokenizer.pad_token_id)
        self.pos_embed = nn.Embedding(config['max_seq_len'], config['hidden_dim'])
        self.seg_embed = nn.Embedding(2, config['hidden_dim'])
        self.layer_norm = nn.LayerNorm(config['hidden_dim'], eps=1e-12)
        self.dropout = nn.Dropout(config['dropout_prob'])

    def forward(self, input_ids, segment_ids):
        seq_len = input_ids.size(1)

        position_ids = torch.arange(seq_len, dtype=torch.long, device=input_ids.device).unsqueeze(0)

        tok_embeds = self.token_embed(input_ids)
        pos_embeds = self.pos_embed(position_ids)
        seg_embeds = self.seg_embed(segment_ids)

        summed_embeds = tok_embeds + pos_embeds + seg_embeds

        normed_embeds = self.layer_norm(summed_embeds)
        dropped_embeds = self.dropout(normed_embeds)

        return dropped_embeds

class MultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.hidden_dim = config['hidden_dim']
        self.num_heads = config['num_heads']
        self.head_dim = self.hidden_dim // self.num_heads

        if self.hidden_dim % self.num_heads != 0:
            raise ValueError("Hidden dimension must be divisible by number of heads.")

        self.q_linear = nn.Linear(self.hidden_dim, self.hidden_dim)
        self.k_linear = nn.Linear(self.hidden_dim, self.hidden_dim)
        self.v_linear = nn.Linear(self.hidden_dim, self.hidden_dim)
        self.out_linear = nn.Linear(self.hidden_dim, self.hidden_dim)
        self.dropout = nn.Dropout(config['dropout_prob'])

    def forward(self, query, key, value, attention_mask=None):
        batch_size = query.size(0)

        q = self.q_linear(query)
        k = self.k_linear(key)
        v = self.v_linear(value)

        q = q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)

        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)

        if attention_mask is not None:
            scores = scores.masked_fill(attention_mask == 0, -1e9)

        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)

        context = torch.matmul(attention_weights, v)

        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.hidden_dim)

        output = self.out_linear(context)
        return output

class FeedForward(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.linear1 = nn.Linear(config['hidden_dim'], config['ff_dim'])
        self.linear2 = nn.Linear(config['ff_dim'], config['hidden_dim'])
        self.activation = nn.GELU()
        self.dropout = nn.Dropout(config['dropout_prob'])

    def forward(self, x):
        x = self.linear1(x)
        x = self.activation(x)
        x = self.dropout(x)
        x = self.linear2(x)
        return x

class TransformerEncoderLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.self_attn = MultiHeadAttention(config)
        self.ffn = FeedForward(config)
        self.norm1 = nn.LayerNorm(config['hidden_dim'])
        self.norm2 = nn.LayerNorm(config['hidden_dim'])
        self.dropout1 = nn.Dropout(config['dropout_prob'])
        self.dropout2 = nn.Dropout(config['dropout_prob'])

    def forward(self, x, attention_mask=None):
        attn_output = self.self_attn(x, x, x, attention_mask)
        x = self.norm1(x + self.dropout1(attn_output))

        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout2(ffn_output))

        return x

class BERT(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.embeddings = BertEmbeddings(config)
        self.encoder_layers = nn.ModuleList([
            TransformerEncoderLayer(config) for _ in range(config['num_layers'])
        ])

    def forward(self, input_ids, segment_ids, attention_mask=None):
        if attention_mask is not None:
            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
            extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0
        else:
            extended_attention_mask = None

        hidden_states = self.embeddings(input_ids, segment_ids)

        for layer in self.encoder_layers:
            hidden_states = layer(hidden_states, extended_attention_mask)

        return hidden_states

class BertMLMHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config['hidden_dim'], config['hidden_dim'])
        self.gelu = nn.GELU()
        self.layer_norm = nn.LayerNorm(config['hidden_dim'])

        self.decoder = nn.Linear(config['hidden_dim'], config['vocab_size'], bias=False)
        self.bias = nn.Parameter(torch.zeros(config['vocab_size']))
        self.decoder.bias = self.bias

    def forward(self, hidden_states):
        x = self.dense(hidden_states)
        x = self.gelu(x)
        x = self.layer_norm(x)
        x = self.decoder(x)
        return x

    def tie_weights(self, embedding_layer):
        self.decoder.weight = embedding_layer.weight

class BertNSPHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.pooler_dense = nn.Linear(config['hidden_dim'], config['hidden_dim'])
        self.pooler_act = nn.Tanh()
        self.classifier = nn.Linear(config['hidden_dim'], 2)

    def forward(self, sequence_output):
        cls_token_output = sequence_output[:, 0]

        pooled_output = self.pooler_dense(cls_token_output)
        pooled_output = self.pooler_act(pooled_output)

        logits = self.classifier(pooled_output)
        return logits

class BertForPretraining(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.bert = BERT(config)
        self.mlm_head = BertMLMHead(config)
        self.nsp_head = BertNSPHead(config)

        self.mlm_head.tie_weights(self.bert.embeddings.token_embed)

    def forward(self, input_ids, segment_ids, attention_mask):
        sequence_output = self.bert(input_ids, segment_ids, attention_mask)

        mlm_logits = self.mlm_head(sequence_output)
        nsp_logits = self.nsp_head(sequence_output)

        return mlm_logits, nsp_logits


print("Model architecture defined successfully.")


class WikiText2Dataset(Dataset):
    def __init__(self, split='train', limit_size=None):
        raw_dataset = datasets.load_dataset('wikitext', 'wikitext-2-v1', split=split)

        self.sentences = []
        print(f"Loading and tokenizing {split} data...")

        for doc in tqdm(raw_dataset['text']):
            if doc.strip() and not doc.strip().startswith('='):
                self.sentences.extend(nltk.sent_tokenize(doc.strip()))

        self.sentences = [s for s in self.sentences if len(s.split()) > 3]

        if limit_size:
            self.sentences = self.sentences[:limit_size]

        print(f"Loaded {len(self.sentences)} sentences.")

    def __len__(self):
        return len(self.sentences) - 1

    def __getitem__(self, idx):
        sent_a = self.sentences[idx]

        if random.random() > 0.5:
            sent_b = self.sentences[idx + 1]
            nsp_label = 0
        else:
            rand_idx = random.randint(0, len(self.sentences) - 1)
            while rand_idx == idx or rand_idx == idx + 1:
                rand_idx = random.randint(0, len(self.sentences) - 1)
            sent_b = self.sentences[rand_idx]
            nsp_label = 1

        return sent_a, sent_b, nsp_label

def create_mlm_training_data(inputs, config):
    input_ids = inputs['input_ids'].clone()
    mlm_labels = torch.full_like(input_ids, -100)

    prob_matrix = torch.full_like(input_ids, config['mask_prob'], dtype=torch.float)

    special_tokens_mask = (input_ids == tokenizer.cls_token_id) | \
                          (input_ids == tokenizer.sep_token_id) | \
                          (input_ids == tokenizer.pad_token_id)
    prob_matrix.masked_fill_(special_tokens_mask, 0.0)

    mask = torch.bernoulli(prob_matrix).bool()
    mlm_labels[mask] = input_ids[mask]

    mask_80 = mask & (torch.bernoulli(torch.full_like(input_ids, 0.8, dtype=torch.float)).bool())
    input_ids[mask_80] = tokenizer.mask_token_id

    mask_10_indices = torch.where(mask & ~mask_80)[0]
    num_to_rand = int(len(mask_10_indices) * 0.5)
    if num_to_rand > 0:
        rand_indices = mask_10_indices[torch.randperm(len(mask_10_indices))[:num_to_rand]]
        random_tokens = torch.randint(0, config['vocab_size'], (num_to_rand,), dtype=torch.long)
        input_ids.view(-1)[rand_indices] = random_tokens

    return input_ids, mlm_labels

def collate_fn(batch, config):
    sent_a_list, sent_b_list, nsp_labels_list = zip(*batch)

    inputs = tokenizer(
        list(sent_a_list),
        list(sent_b_list),
        padding='max_length',
        truncation=True,
        max_length=config['max_seq_len'],
        return_tensors='pt'
    )

    mlm_input_ids, mlm_labels = create_mlm_training_data(inputs, config)

    return {
        "input_ids": mlm_input_ids,
        "segment_ids": inputs['token_type_ids'],
        "attention_mask": inputs['attention_mask'],
        "mlm_labels": mlm_labels,
        "nsp_labels": torch.tensor(nsp_labels_list, dtype=torch.long)
    }

print("\n--- Initializing Datasets ---")
train_dataset = WikiText2Dataset(split='train', limit_size=CONFIG['limit_dataset_size'])
val_dataset = WikiText2Dataset(split='validation', limit_size=CONFIG['limit_dataset_size'] // 10)

collate_with_config = lambda batch: collate_fn(batch, CONFIG)

train_dataloader = DataLoader(
    train_dataset,
    batch_size=CONFIG['batch_size'],
    shuffle=True,
    collate_fn=collate_with_config
)
val_dataloader = DataLoader(
    val_dataset,
    batch_size=CONFIG['batch_size'],
    shuffle=False,
    collate_fn=collate_with_config
)

print("Datasets and DataLoaders created.")


model = BertForPretraining(CONFIG).to(device)
optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])
mlm_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)
nsp_loss_fn = nn.CrossEntropyLoss()

print("Model, Optimizer, and Loss Functions initialized.")


print("\n--- Starting Training ---")
model.train()
train_step = 0
train_loop_done = False

while not train_loop_done:
    for batch in train_dataloader:
        if train_step >= CONFIG['num_train_steps']:
            train_loop_done = True
            break

        input_ids = batch['input_ids'].to(device)
        segment_ids = batch['segment_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        mlm_labels = batch['mlm_labels'].to(device)
        nsp_labels = batch['nsp_labels'].to(device)

        optimizer.zero_grad()

        mlm_logits, nsp_logits = model(input_ids, segment_ids, attention_mask)

        mlm_loss = mlm_loss_fn(mlm_logits.view(-1, CONFIG['vocab_size']), mlm_labels.view(-1))
        nsp_loss = nsp_loss_fn(nsp_logits.view(-1, 2), nsp_labels.view(-1))
        total_loss = mlm_loss + nsp_loss

        total_loss.backward()
        optimizer.step()

        if (train_step + 1) % 20 == 0:
            print(f"Step {train_step+1}/{CONFIG['num_train_steps']} | "
                  f"Total Loss: {total_loss.item():.4f} | "
                  f"MLM Loss: {mlm_loss.item():.4f} | "
                  f"NSP Loss: {nsp_loss.item():.4f}")

        train_step += 1

print("--- Training Finished ---")


print("\n--- Starting Evaluation ---")
model.eval()

total_nsp_correct = 0
total_nsp_count = 0
eval_step = 0

with torch.no_grad():
    for batch in val_dataloader:
        if eval_step >= CONFIG['num_eval_steps']:
            break

        input_ids = batch['input_ids'].to(device)
        segment_ids = batch['segment_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        nsp_labels = batch['nsp_labels'].to(device)

        _, nsp_logits = model(input_ids, segment_ids, attention_mask)

        preds = torch.argmax(nsp_logits, dim=1)
        total_nsp_correct += (preds == nsp_labels).sum().item()
        total_nsp_count += nsp_labels.size(0)

        eval_step += 1

if total_nsp_count > 0:
    nsp_accuracy = (total_nsp_correct / total_nsp_count) * 100
    print(f"\nNext Sentence Prediction (NSP) Accuracy on validation set: {nsp_accuracy:.2f}%")
else:
    print("\nNo validation steps were run. NSP accuracy not calculated.")

print("\n--- Demonstrating MLM Prediction ---")

text = f"The capital of France is [MASK]."

inputs = tokenizer(text, return_tensors='pt').to(device)
input_ids = inputs['input_ids']
mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1].item()

with torch.no_grad():
    mlm_logits, _ = model(input_ids=input_ids,
                            segment_ids=inputs['token_type_ids'],
                            attention_mask=inputs['attention_mask'])

mask_logits = mlm_logits[0, mask_token_index, :]

top_5_tokens = torch.topk(mask_logits, 5, dim=0).indices.tolist()
predicted_tokens = tokenizer.convert_ids_to_tokens(top_5_tokens)

print(f"Input: '{text}'")
print(f"Top 5 predictions for [MASK]:")
for i, token in enumerate(predicted_tokens):
    print(f"  {i+1}. {token}")

print("\n--- Script Finished ---")

